\section{}
\label{sec:p1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{a)}
The '\textit{seismic.dat}' data contains seismic data on a regular grid $(75\times75)$, $L_D$, denoted as $\{d(\vect{x}) \ssep \vect{x}\in L_D\} \ssep d(\vect{x})\in \R$, and visually represented by the map in Figure \ref{fig:data}.
The seismic data has the likelihood model given by the equation
\begin{equation}
    [d_i\given\vect{l}] = \begin{cases}
                                0.02 + U_i, & l_i = 0 \Rightarrow \mathrm{sand} \\
                                0.08 + U_i, & l_i = 1 \Rightarrow \mathrm{shale}
                             \end{cases}
                            , i = 1, 2, ..., n,
    \label{eq:likelihood}
\end{equation}

where $U_i \overset{\mathrm{iid}}{\sim}\N\{0,0.06^2\}$. This means that we can rewrite Equation \ref{eq:likelihood} to 
\begin{equation}
    [d_i\given\vect{l}] = \begin{cases}
                                A_i\overset{\mathrm{iid}}{\sim}\N\{0.02,0.06^2\}, & l_i = 0 \Rightarrow \mathrm{sand} \\
                                B_i\overset{\mathrm{iid}}{\sim}\N\{0.08,0.06^2\}, & l_i = 1 \Rightarrow \mathrm{shale}
                             \end{cases}
                            , i = 1, 2, ..., n.
    \label{eq:likelihood2}
\end{equation}
The likelihood model $p(\vect{d}\given \vect{l})$ is then given by 
\begin{equation}
    \begin{array}{rcl}
        [\vect{d}\given\vect{l}] \sim p(\vect{d}\given\vect{l}) & = & \prod\limits_{i=1}^n p(d_i\given l_i) \\
        & = & (0.0072\pi)^{-n/2} \prod\limits_{i=1}^n l_i \exp\left\{\frac{(d_i - 0.08)^2}{0.0072} \right\} + (1-l_i) \exp\left\{\frac{(d_i - 0.02)^2}{0.0072} \right\} \\
         & = & (0.0072\pi)^{-n/2} \exp\left\{\frac{1}{0.0072} \left[\sum\limits_{i \st l_i = 0} (d_i - 0.02)^2 + \sum\limits_{i \st l_i = 1} (d_i - 0.08)^2\right]\right\} \, .
    \end{array}
    \label{eq:likelihood3}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/seismic_data.pdf}
    \caption{Map of the seismic data $d(\vect{x})$ on a grid $\matr{L_D}$}
    \label{fig:data}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{b)}
We consider a uniform form prior $\prob(\vect{l}) = const$. We have $\prob(\vect{l}) = \prod_{i=1}^n \prob(l_i)$ and $\prob(l_i) = 1/2$. Assuming the data are collected independently, i.e. $\prob(\vect{d}) = \prod_{i=1}^n \prob(d_i)$, we get the posterior
%
\begin{align*}
    \prob(\vect{l} \given \vect{d}) &= \frac{\prob(\vect{d} \given \vect{l}) \prob(\vect{l})}{\prob(\vect{d})} \\
    &= \prod_{i=1}^n \frac{\prob(d_i \given l_i) \prob(l_i)}{\prob(d_i)} \\
    &= \prod_{i=1}^n \prob(l_i \given d_i) \, ,
\end{align*}
%
where $\prob(d_i) = 1/2 \cdot [\prob(d_i \given l_i=0) + \prob(d_i \given l_i=1)]$. Thus the values of $l_i$, $i = 1, \dots, n$ are independent Bernoulli random variables with parameters $p_i = \prob(l_i \given d_i)$. The expectation of the $i$th element is $p_i$ and the variance is $p_i(1-p_i)$. 

\figref{fig:b_sims}, \figref{fig:b_mean} and \figref{fig:b_var} show 10 simulations from the posterior distribution, the posterior mean and the posterior variance, respectively. We see that the mean is high where the measurements are high and low where the measurements are low. The variance is low both where the measurements are high and low, while it is larger in ares where the measurements are not clearly indicating whether the is shale or sand in an area. The posterior mean $\E[\vect{l} | \vect{d}]$ is the probability that an area contains shale (1).

\begin{figure}
    \centering
    \includegraphics{figures/b_sims.pdf}
    \caption{10 simulations from the posterior given in b).}
    \label{fig:b_sims}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/b_mean.pdf}
    \caption{Posterior mean as derived in b).}
    \label{fig:b_mean}
\end{figure}

\begin{figure}
    \centering
    \includegraphics{figures/b_var.pdf}
    \caption{Posterior variance as derived in b).}
    \label{fig:b_var}
\end{figure}

The maximum marginal posterior predictor $\text{MMAP}\left\{\vect{l}|\vect{d}\right\}$ is the vector of values maximizing $p(l_i \given d_i)$, $i = 1, \dots, n$, which are 1 where $p(l_i \given d_i) > 0.5$ and 0 else. A plot of the MMAP is shown in \figref{fig:b_mmap}.

\begin{figure}
    \centering
    \includegraphics{figures/b_mmap.pdf}
    \caption{Maximum marginal posterior predictor as derived in b).}
    \label{fig:b_mmap}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{c)}
We will now consider a Markov RF prior model for $\{l(\vect{x}; \vect{x} \in L_D\}$, represented by the vector $\vect{l}$.
This can be specified by the Markov formulation or Gibbs formulation. The Markov formulation for the prior is 

\begin{equation}
    p(l_i|l_j; j \in \vect{n}_i) = const_M \times \exp{\{\beta \sum_{j\in \vect{n}_i} I(l_i = l_j)\}}; \quad i = 1,2,...n,
\end{equation}
where $\vect{n}_i$ is the neighborhood of grid node $i$, consisting of the four closest grid nodes and $I(.)$ is the indicator function. This is the Ising model. 

This neighboring system corresponds to a clique system where each clique consists of two neighboring nodes, and we write it
\begin{equation}
    \vect{C}_L = \{<i,j>; i,j \in L_D\}
\end{equation}

Then the associated Gibbs formulation of the prior is

\begin{align}
    p(\vect{l}) &= const_G \times \prod_{<i,j>} \exp{\{\beta I(l_i = l_j)\}}\\
    & = const_G \times \exp{\{\beta \sum_{<i,j>} I(l_i = l_j)\}}.
\end{align}

Further, we can find expressions for the posterior distributions $p(\vect{l}|\vect{d})$ and $p(l_i | \vect{d}, \vect{l}_{-i})$, $i = 1, \dots, n$ and index $-i$ meaning all nodes except $i$. 

\begin{align}
\begin{split}
    p(l_i | \vect{d}, \vect{l}_{-i}) &= \frac{p(\vect{d}|\vect{l})p(l_i|\vect{l}_{-i})}{p(\vect{d}|\vect{l}_{-i})}  = \frac{\prod_{k} p(d_k|l_k)\times p(l_i|l_j; j\in \vect{n}_i)}{p(d_i|\vect{l}_{-i})\prod_{k\neq i}p(d_k|l_k)}\\
    & = \frac{p(d_i|l_i)p(l_i|l_j; j\in \vect{n}_i)}{p(d_i|\vect{l}_{-i})},
\end{split}
\label{eq:full_cond}
\end{align}

where $p(d_i|\vect{l}_{-i}) = \sum_{l_i^*\in \{0,1\}} p(d_i|l_i^*)p(l_i^*|l_j; j\in \vect{n}_i)$ and $p(d_i|l_i)$ and $p(l_i|l_j; j\in \vect{n}_i)$ have been specified previously. 

\begin{equation*}
    p(\vect{l}|\vect{d}) = \frac{p(\vect{d}|\vect{l})p(\vect{l})}{p(\vect{d})} = \frac{p(\vect{d}|\vect{l})p(\vect{l})}{\sum_{\vect{l^*}}p(\vect{d}|\vect{l^*})p(\vect{l^*})} ,
\end{equation*}

so that  $p(\vect{d})$ is a sum of all $2^n$ possible configurations of the $n$-vector $\vect{l}$.

The aim is to produce realizations from the posterior distribution $p(\vect{l}|\vect{d})$ and find the posterior mean $\E[\vect{l} | \vect{d}]$, variance $\Var\{\vect{l} | \vect{d}\}$ and alternative prediction $\textrm{MMAP}\{\vect{l} | \vect{d}\}$. We will have to use an MCMC sampler to generate realizations from the posterior, and then $\E[\vect{l} | \vect{d}]$, $\Var\{\vect{l} | \vect{d}\}$ and $\textrm{MMAP}\{\vect{l} | \vect{d}\} = \textrm{argmax}_{l_i}\{ p(l_i|d_i\}$ will be estimated numerically. The latter prediction is calculated simply by counting the frequency of $0$ and $1$ in each position over the posterior realizations, and assigning the one with the highest frequency.

We want to use observations from a comparable domain $D_c \subset \R^2$ to train the prior model, i.e estimate $\beta$. The data from that area is displayed in Figure \ref{fig:complit_data}. Here we have exact observations, and call these $\vect{l}^o$.

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/complit_data.pdf}
    \caption{Data from comparable domain $D_c$}
    \label{fig:complit_data}
\end{figure}


We use a maximum psedo-likelihood procedure to estimate $\beta$. We would want to find Maximal Marginal Likelihood (MML) estimate by maximizing $p(\vect{d}; \beta)$, but we have seen that $p(\vect{d})$ is very computer demanding. Thus, we instead maximize the Marginal pseudo-Likelihood (MpL), 

\begin{align*}
     &\prod_{i=1}^n p(d_i, d_j; j \in \vect{n}_i)\\
    &\approx \prod_{i=1}^n \sum_{[l_i^*, l_j^*; j\in \vect{n}_i] \in \{0,1\}} \prod_{j=i, j\in \vect{n}_i} p(d_i|l_i^*)p(l_i^*|l_j^*, j\in \vect{n}_i; \beta)\\
    &:= \hat{p}(\vect{d}; \beta).
\end{align*}
 
Since we have exact observations, this simplifies to 
\begin{equation*}
    \hat{p}(\vect{d}; \beta) = \hat{p}(\vect{l}^o; \beta) = \prod_{i=1}^n p(l_i^o|l_j^o, j\in \vect{n}_i; \beta), 
\end{equation*}

with 
\begin{equation*}
    p(l_i^o|l_j^o, j\in \vect{n}_i; \beta) = \frac{\exp{\{\beta \sum_{j\in \vect{n}_i} I(l_i = l_j)\}}}{\sum_{l_i^*\in \{0,1\}}\exp{\{\beta \sum_{j\in \vect{n}_i} I(l_i^* = l_j)\}}}.
\end{equation*}

Thus, 
\begin{equation*}
    \hat\beta = \textrm{argmax}_{\beta} \{\log(\hat{p}(\vect{l}^o; \beta))\},
\end{equation*}
which we find using the optim() function in R to be $\hat\beta = 1.72$.

We now want to generate realizations from the posterior distribution $p(\vect{l}|\vect{d}; \hat\beta)$. We do this by using MCMC/Gibbs sampler. To avoid boundary problems for the cliques, we use wrapping boundary conditions, which means that we let the system vertically and horizontally periodic, so that when we need a value that is located outside the grid, we use the value on the other side of the grid, vertically or horizontally.

In the Gibbs sampler, we propose a value (either $0$ or $1$) for one location at a time, from $p(l_i|\vect{d}, \vect{l}_{-i})$ as given in equation \eqref{eq:full_cond}.
Because of the properties of this full conditional distribution, the acceptance probability is always 1, so the step is always accepted. It is obvious that we will have to propose to change quite a few locations before the the chain can converge and between realizations that we can assume are independent. Therefore, realizations of the chain is only saved for every $n$th proposal. This will be called one \textit{iteration}. I.e, when we run the MCMC $3000$ steps, $n\cdot 3000$ locations have been proposed to change.

To assess the question of convergence, we display the fraction of 0 and 1 in the Markov Chain. This is shown in Figure \ref{fig:MCMC_frac}.

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/.pdf}
    \caption{}
    \label{fig:MCMC_frac}
\end{figure}

Realizations
\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/.pdf}
    \caption{}
    \label{fig:MCMC_realiz}
\end{figure}

Plots of mean etc

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/.pdf}
    \caption{}
    \label{fig:MCMC_mean}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/.pdf}
    \caption{}
    \label{fig:MCMC_var}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.95]{figures/.pdf}
    \caption{}
    \label{fig:MCMC_MMAP}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{d)}

